{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.naive_bayes import  MultinomialNB \n",
    "from sklearn.svm import  SVC\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import  classification_report, accuracy_score, confusion_matrix\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cleaned_language_detection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nature broadest natural physical material worl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nature refer phenomenon physical world life ge...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the study nature large part science</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>although human part nature human activity unde...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the word nature borrowed old french nature der...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language\n",
       "0  nature broadest natural physical material worl...  English\n",
       "1  nature refer phenomenon physical world life ge...  English\n",
       "2                the study nature large part science  English\n",
       "3  although human part nature human activity unde...  English\n",
       "4  the word nature borrowed old french nature der...  English"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"Text\"]\n",
    "y = data[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets encode our Target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's vectorise out  the code and make it more efficient for the model to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna('')\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets define a fucntion that will help in traning all desired models.\n",
    "def data_model():\n",
    "    models = {\"log_R\": LogisticRegression(),\n",
    "              \"nb\": MultinomialNB(),\n",
    "              \"supprt\": SVC(),\n",
    "              \"rand_F\": RandomForestClassifier()\n",
    "              }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = data_model()\n",
    "\n",
    "for name,model in model_train.items():\n",
    "    model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION:log_R\n",
      "ACCURACY:0.8411657559198543\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.59      0.73       146\n",
      "           1       0.99      0.67      0.80       194\n",
      "           2       0.55      0.99      0.71       481\n",
      "           3       0.96      0.95      0.96       316\n",
      "           4       0.99      0.71      0.83       140\n",
      "           5       1.00      0.76      0.86       119\n",
      "           6       1.00      0.90      0.95        20\n",
      "           7       1.00      0.83      0.91       249\n",
      "           8       0.97      0.88      0.93       240\n",
      "           9       1.00      0.85      0.92       199\n",
      "          10       0.92      0.85      0.88       262\n",
      "          11       0.96      0.85      0.90       222\n",
      "          12       1.00      0.68      0.81       157\n",
      "\n",
      "    accuracy                           0.84      2745\n",
      "   macro avg       0.95      0.81      0.86      2745\n",
      "weighted avg       0.90      0.84      0.85      2745\n",
      "\n",
      "CONFUSION MATRIX:\n",
      " [[ 86   0  50   1   0   0   0   1   0   0   0   8   0]\n",
      " [  0 130  61   3   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 478   3   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1  14 301   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  40   0 100   0   0   0   0   0   0   0   0]\n",
      " [  0   0  29   0   0  90   0   0   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0  18   0   0   0   0   0   0]\n",
      " [  0   0  32   3   0   0   0 207   1   0   6   0   0]\n",
      " [  0   0  15   0   0   0   0   0 212   0  13   0   0]\n",
      " [  0   0  28   1   0   0   0   0   0 170   0   0   0]\n",
      " [  0   0  33   0   1   0   0   0   5   0 223   0   0]\n",
      " [  3   0  31   0   0   0   0   0   0   0   0 188   0]\n",
      " [  0   0  50   1   0   0   0   0   0   0   0   0 106]]\n",
      "HERE WE GO............\n",
      "EVALUATION:nb\n",
      "ACCURACY:0.8029143897996357\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.46      0.63       146\n",
      "           1       1.00      0.56      0.72       194\n",
      "           2       0.49      1.00      0.65       481\n",
      "           3       0.96      0.97      0.96       316\n",
      "           4       1.00      0.69      0.81       140\n",
      "           5       1.00      0.73      0.84       119\n",
      "           6       1.00      0.60      0.75        20\n",
      "           7       1.00      0.82      0.90       249\n",
      "           8       0.97      0.88      0.92       240\n",
      "           9       1.00      0.82      0.90       199\n",
      "          10       0.95      0.85      0.90       262\n",
      "          11       0.97      0.82      0.89       222\n",
      "          12       1.00      0.43      0.60       157\n",
      "\n",
      "    accuracy                           0.80      2745\n",
      "   macro avg       0.95      0.74      0.81      2745\n",
      "weighted avg       0.90      0.80      0.81      2745\n",
      "\n",
      "CONFUSION MATRIX:\n",
      " [[ 67   0  71   2   0   0   0   0   0   0   0   6   0]\n",
      " [  0 108  83   3   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 479   2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  10 306   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  44   0  96   0   0   0   0   0   0   0   0]\n",
      " [  0   0  32   0   0  87   0   0   0   0   0   0   0]\n",
      " [  0   0   8   0   0   0  12   0   0   0   0   0   0]\n",
      " [  0   0  41   1   0   0   0 203   1   0   3   0   0]\n",
      " [  0   0  18   4   0   0   0   0 210   0   8   0   0]\n",
      " [  0   0  36   0   0   0   0   0   0 163   0   0   0]\n",
      " [  0   0  35   0   0   0   0   0   3   0 224   0   0]\n",
      " [  0   0  40   0   0   0   0   0   0   0   0 182   0]\n",
      " [  0   0  87   1   0   0   0   0   2   0   0   0  67]]\n",
      "HERE WE GO............\n",
      "EVALUATION:supprt\n",
      "ACCURACY:0.8025500910746812\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.48      0.64       146\n",
      "           1       0.96      0.55      0.70       194\n",
      "           2       0.49      0.99      0.66       481\n",
      "           3       0.97      0.94      0.96       316\n",
      "           4       1.00      0.65      0.79       140\n",
      "           5       1.00      0.71      0.83       119\n",
      "           6       1.00      0.90      0.95        20\n",
      "           7       1.00      0.79      0.88       249\n",
      "           8       0.98      0.85      0.91       240\n",
      "           9       1.00      0.83      0.91       199\n",
      "          10       0.91      0.85      0.88       262\n",
      "          11       0.98      0.81      0.88       222\n",
      "          12       1.00      0.56      0.72       157\n",
      "\n",
      "    accuracy                           0.80      2745\n",
      "   macro avg       0.94      0.76      0.82      2745\n",
      "weighted avg       0.89      0.80      0.81      2745\n",
      "\n",
      "CONFUSION MATRIX:\n",
      " [[ 70   1  70   1   0   0   0   0   0   0   0   4   0]\n",
      " [  0 107  83   4   0   0   0   0   0   0   0   0   0]\n",
      " [  0   2 477   2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1  17 298   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  49   0  91   0   0   0   0   0   0   0   0]\n",
      " [  0   0  35   0   0  84   0   0   0   0   0   0   0]\n",
      " [  0   0   2   0   0   0  18   0   0   0   0   0   0]\n",
      " [  0   0  41   3   0   0   0 197   1   0   7   0   0]\n",
      " [  0   0  19   0   0   0   0   0 205   0  16   0   0]\n",
      " [  0   0  34   0   0   0   0   0   0 165   0   0   0]\n",
      " [  0   0  34   0   0   0   0   0   4   0 224   0   0]\n",
      " [  2   0  41   0   0   0   0   0   0   0   0 179   0]\n",
      " [  0   0  69   0   0   0   0   0   0   0   0   0  88]]\n",
      "HERE WE GO............\n",
      "EVALUATION:rand_F\n",
      "ACCURACY:0.8058287795992713\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.65      0.73       146\n",
      "           1       0.95      0.67      0.79       194\n",
      "           2       0.53      0.97      0.69       481\n",
      "           3       0.91      0.86      0.89       316\n",
      "           4       0.98      0.81      0.89       140\n",
      "           5       1.00      0.83      0.91       119\n",
      "           6       1.00      0.95      0.97        20\n",
      "           7       0.98      0.76      0.85       249\n",
      "           8       0.92      0.81      0.86       240\n",
      "           9       1.00      0.79      0.88       199\n",
      "          10       0.87      0.77      0.82       262\n",
      "          11       0.93      0.73      0.82       222\n",
      "          12       1.00      0.72      0.84       157\n",
      "\n",
      "    accuracy                           0.81      2745\n",
      "   macro avg       0.91      0.79      0.84      2745\n",
      "weighted avg       0.87      0.81      0.82      2745\n",
      "\n",
      "CONFUSION MATRIX:\n",
      " [[ 95   0  37   2   0   0   0   0   0   0   1  11   0]\n",
      " [  2 130  59   2   0   0   0   0   1   0   0   0   0]\n",
      " [  0   4 467   6   1   0   0   0   2   0   0   1   0]\n",
      " [  0   2  34 273   0   0   0   2   0   0   5   0   0]\n",
      " [  1   0  25   0 113   0   0   0   0   0   0   1   0]\n",
      " [  0   0  20   0   0  99   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0  19   0   0   0   0   0   0]\n",
      " [  0   0  43   4   0   0   0 188   5   0   9   0   0]\n",
      " [  0   0  24   9   0   0   0   1 194   0  12   0   0]\n",
      " [  0   0  41   1   0   0   0   0   0 157   0   0   0]\n",
      " [  0   0  47   2   1   0   0   1  10   0 201   0   0]\n",
      " [ 17   1  38   1   0   0   0   0   0   0   2 163   0]\n",
      " [  0   0  44   0   0   0   0   0   0   0   0   0 113]]\n",
      "HERE WE GO............\n"
     ]
    }
   ],
   "source": [
    "for name,model in model_train.items():\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f'EVALUATION:{name}')\n",
    "    print(f'ACCURACY:{accuracy_score(y_test,predictions)}')\n",
    "    print(f'CLASSIFICATION REPORT:\\n {classification_report(y_test,predictions)}')\n",
    "    print(f'CONFUSION MATRIX:\\n {confusion_matrix(y_test,predictions)}')\n",
    "    print(\"HERE WE GO............\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters on Logicstic Regression.\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_R = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 10],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.01, 0.1, 1, 10],\n",
       "                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;, &#x27;newton-cg&#x27;, &#x27;lbfgs&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10],\n",
       "                         'solver': ['liblinear', 'newton-cg', 'lbfgs']},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid ={\n",
    "    'solver':['liblinear','newton-cg','lbfgs'],\n",
    "\n",
    "    'C':[0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(log_R,param_grid,cv=3,n_jobs=-1,verbose=2,scoring='accuracy')\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the best parameter:{'C': 10, 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "best_param = grid_search.best_params_\n",
    "print(f'this is the best parameter:{best_param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=10, solver='liblinear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = LogisticRegression(**best_param)\n",
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST MODEL EVALUTION\n",
      "ACCURACY:0.9045537340619307\n",
      "CLASSIFICTION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85       146\n",
      "           1       0.96      0.82      0.88       194\n",
      "           2       0.71      0.98      0.82       481\n",
      "           3       0.97      0.96      0.96       316\n",
      "           4       0.99      0.85      0.92       140\n",
      "           5       1.00      0.87      0.93       119\n",
      "           6       1.00      0.95      0.97        20\n",
      "           7       1.00      0.90      0.95       249\n",
      "           8       0.96      0.90      0.93       240\n",
      "           9       1.00      0.89      0.94       199\n",
      "          10       0.92      0.89      0.91       262\n",
      "          11       0.95      0.92      0.94       222\n",
      "          12       1.00      0.87      0.93       157\n",
      "\n",
      "    accuracy                           0.90      2745\n",
      "   macro avg       0.95      0.89      0.92      2745\n",
      "weighted avg       0.92      0.90      0.91      2745\n",
      "\n",
      "CONFUSSION MARIX:\n",
      " [[115   2  19   1   0   0   0   1   0   0   0   8   0]\n",
      " [  1 159  32   2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   2 473   4   0   0   0   0   0   0   1   1   0]\n",
      " [  0   2  11 303   0   0   0   0   0   0   0   0   0]\n",
      " [  1   1  18   0 119   0   0   0   0   0   0   1   0]\n",
      " [  0   0  16   0   0 103   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0  19   0   0   0   0   0   0]\n",
      " [  0   0  17   1   0   0   0 224   2   0   5   0   0]\n",
      " [  0   0  11   0   0   0   0   0 216   0  13   0   0]\n",
      " [  0   0  20   1   0   0   0   0   0 178   0   0   0]\n",
      " [  0   0  22   0   1   0   0   0   6   0 233   0   0]\n",
      " [  9   0   9   0   0   0   0   0   0   0   0 204   0]\n",
      " [  0   0  20   0   0   0   0   0   0   0   0   0 137]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = best_model.predict(X_test)\n",
    "\n",
    "print('BEST MODEL EVALUTION')\n",
    "print(f'ACCURACY:{accuracy_score(y_test, y_predict)}')\n",
    "print(f'CLASSIFICTION REPORT:\\n {classification_report(y_test, y_predict)}')\n",
    "print(f'CONFUSSION MARIX:\\n {confusion_matrix(y_test, y_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's Test our Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(text):\n",
    "    vector = tfidf.transform([text]).toarray()\n",
    "    data = best_model.predict(vector)\n",
    "    language_prediction = le.inverse_transform(data)\n",
    "    print('PREDICTING.....')\n",
    "    print('LANGUAGE IS:',language_prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test = \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTING.....\n",
      "LANGUAGE IS: English\n",
      "<function model_test at 0x000001EF50B13560>\n"
     ]
    }
   ],
   "source": [
    "model_test(user_test)\n",
    "print(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_class.pkl','wb')as file:\n",
    "   pickle.dump(best_model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl','wb')as file:\n",
    "    pickle.dump(tfidf,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_encoder.pkl','wb')as file:\n",
    "    pickle.dump(le,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
